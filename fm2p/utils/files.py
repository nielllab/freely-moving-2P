# -*- coding: utf-8 -*-
"""
File read/write utilities.

Functions
---------
open_dlc_h5(dlc_path, h5key=None)
    Open the .h5 file generated by DLC.
write_h5(filename, dic)
    Write a dictionary to an .h5 file.
recursively_save_dict_contents_to_group(h5file, path, dic)
    Recursively save dictionary contents to a group in an .h5 file. This is
    not accessible from outside the module.
recursively_load_dict_contents_from_group(h5file, path)
    Recursively load dictionary contents from a group in an .h5 file. This is
    not accessible from outside the module.
read_h5(filename, ASLIST=False)
    Read an .h5 file in as a dictionary.
read_yaml(path)
    Read a .yaml file.
write_yaml(path, contents)
    Write a .yaml file.


Author: DMM, 2024
"""

import json
import yaml
import h5py
import datetime
import numpy as np
import pandas as pd

import fm2p


def open_dlc_h5(dlc_path, h5key=None):
    """ Open the .h5 file generated by DLC.

    Parameters
    ----------
    dlc_path : str
        Filepath to the DLC .h5 file.
    h5key : str
        The key to the .h5 file. Default is None.

    Returns
    -------
    pts : pd.DataFrame
        Dataframe of the tracked points, where there are three columns
        for each tracked position: _x, _y, and _likelihood. Each row is
        a camera frame from the tracked video.
    pt_loc_names : list
        List of all the named points, excluding the suffix (e.g., '_x').
    """
    
    if h5key is None:
        # read the .hf file when there is no key
        pts = pd.read_hdf(dlc_path)
    
    else:
        # read in .h5 file when there is a key set in corral_files.py
        pts = pd.read_hdf(dlc_path, key=h5key)
    
    # organize columns
    pts.columns = [' '.join(col[:][1:3]).strip() for col in pts.columns.values]
    
    pts = pts.rename(columns={pts.columns[n]: pts.columns[n].replace(' ', '_') for n in range(len(pts.columns))})
    pt_loc_names = pts.columns.values
    return pts, pt_loc_names


def write_h5(filename, dic):
    """ Write a dictionary to an .h5 file.

    The dictionary can only contain values that are of the
    following types: dict, list, numpy.ndarray, or basic scalar
    types (int, float, str, bytes). The hierarchy of the dictionary
    is preserved in the .h5 file that is written. The keys of
    the dictionary can only be type str (not int).

    Modified from https://codereview.stackexchange.com/a/121308

    Parameters
    ----------
    filename : str
        Path to the .h5 file.
    dic : dict
        Dictionary to be saved.
    """

    with h5py.File(filename, 'w') as h5file:

        recursively_save_dict_contents_to_group(h5file, '/', dic)


def recursively_save_dict_contents_to_group(h5file, path, dic):

    if isinstance(dic,dict):
        iterator = dic.items()

    elif isinstance(dic,list):
        iterator = enumerate(dic)

    else:
        ValueError('Cannot save %s type' % type(dic))

    for key, item in iterator:

        if isinstance(dic,list):
            key = str(key)
            
        if isinstance(item, (np.ndarray, np.int16, np.int64, np.float64,
                             int, float, str, bytes, np.float32, np.int32)):
            
            try:
                h5file[path + key] = item
            
            except TypeError:
                if isinstance(item, np.ndarray) and (item.dtype == object):
                    recursively_save_dict_contents_to_group(h5file, path + key + '/', item.item())

        elif isinstance(item, dict) or isinstance(item, list):
            recursively_save_dict_contents_to_group(h5file, path + key + '/', item)

        elif isinstance(item, datetime.datetime):
             h5file[path + key] = fm2p.time2str(item)

        else:
            raise ValueError('Cannot save %s type'%type(item))


def recursively_load_dict_contents_from_group(h5file, path):
    
    ans = {}

    for key, item in h5file[path].items():

        if isinstance(item, h5py._hl.dataset.Dataset):
            ans[key] = item[()]

        elif isinstance(item, h5py._hl.group.Group):
            ans[key] = recursively_load_dict_contents_from_group(h5file,
                                                                 path + key + '/')

    return ans


def read_h5(filename, aslist=False):
    """ Read an .h5 file in as a dictionary.

    Modified from https://codereview.stackexchange.com/a/121308

    Parameters
    ----------
    filename : str
        Path to the .h5 file.
    aslist : bool
        If True, the dictionary will be read in as a list (on the first
        layer). Keys must have been convertable to integers when the file
        was written.
    """
    
    with h5py.File(filename, 'r') as h5file:

        out = recursively_load_dict_contents_from_group(h5file, '/')

        if aslist:

            outl = [None for l in range(len(out.keys()))]

            for key, item in out.items():
                outl[int(key)] = item
            out = outl


        return out
    

def read_yaml(path):
    """ Read a .yaml file.

    Parameters
    ----------
    path : str
        Path to the .yaml file.
    
    Returns
    -------
    contents : dict
        Dictionary of the contents of the .yaml file.
    """

    with open(path, 'r') as infile:
        contents = yaml.load(infile, Loader=yaml.FullLoader)

    return contents


def write_yaml(path, contents):
    """ Write a .yaml file.

    Parameters
    ----------
    path : str
        Path to the .yaml file.
    """

    with open(path, 'w') as outfile:
        yaml.dump(contents, outfile, default_flow_style=False)



def write_group_h5(df, savepath, repair_overflow=False):
    """ Use pandas .to_hdf() method for multiple recordings.
    
    This is just a wrapper function to make sure it is
    handled in the same each time. The dataframe will be split
    by the column 'session'. Each unqiue session will be put
    into its own key of the h5 file.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe to be saved.
    savepath : str
        Path to the .h5 file.

    """

    if repair_overflow:
        df = normalize_for_hdf(fix_overflow_columns(df))

    split_key = 'base_name'
    split_list = df[split_key].unique()

    for i, sname in enumerate(split_list):

        print('Writing block {} of {} (key={})'.format(i+1, len(split_list), sname))
        
        df[df[split_key]==sname].to_hdf(savepath, sname, mode='a')


def get_group_h5_keys(savepath):
    """ Get the keys of a group h5 file.

    This will list the keys (i.e. the session names) of an h5 file
    written by the function write_group_h5 (above). It does not need
    to read the entire file into memory to check these values.

    Parameters
    ----------
    savepath : str
        Path to the .h5 file.
    
    Returns
    -------
    keys : list
        List of keys (i.e. session names) in the h5 file.

    """

    with pd.HDFStore(savepath) as hdf:
        
        keys = [k.replace('/','') for k in hdf.keys()]

    return keys


def read_group_h5(path, keys=None):
    """ Read a group h5 file.

    This will read in a group h5 file written by the function
    write_group_h5 (above). It will read in all keys and stack
    them into a single dataframe. Alternatively, you can specify
    a list of keys to read in from the keys present, and only those
    recordings will be read into memory and stacked together.
    
    Parameters
    ----------
    path : str
        Path to the .h5 file.
    keys : list or str (optional).
        List of keys (i.e. session names) in the h5 file. If None,
        all keys will be read in.
    
    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing all data from the h5 file.

    """

    if type(keys) == str:

        df = pd.read_hdf(path, keys)

        return df
    
    if keys is None:

        keys = get_group_h5_keys(path)

    dfs = []
    for k in sorted(keys):

        _df = pd.read_hdf(path, k) 
        dfs.append(_df)

    df = pd.concat(dfs)

    return df

def find_hdf_overflow_columns(df):

    bad_cols = []

    for col in df.columns:
        s = df[col]

        if pd.api.types.is_integer_dtype(s):
            if (s.min() < np.iinfo(np.int64).min) or (s.max() > np.iinfo(np.int64).max):
                bad_cols.append(col)

        elif pd.api.types.is_object_dtype(s):
            if s.apply(lambda x: isinstance(x, int) and (x > np.iinfo(np.int64).max or x < np.iinfo(np.int64).min)).any():
                bad_cols.append(col)
                
    return bad_cols


def fix_overflow_columns(df):

    bad_cols = find_hdf_overflow_columns(df)

    if not bad_cols:
        return df

    df_fixed = df.copy()

    for col in bad_cols:
        print(f"Converting column '{col}' to string (values exceed int64 range).")
        df_fixed[col] = np.clip(df_fixed[col], np.iinfo(np.int64).min, np.iinfo(np.int64).max).astype("int64")

    return df_fixed

def normalize_for_hdf(df):

    INT64_MIN, INT64_MAX = np.iinfo(np.int64).min, np.iinfo(np.int64).max

    df_fixed = df.copy()

    # --- Handle index ---
    try:
        if df_fixed.index.dtype == object or pd.api.types.is_integer_dtype(df_fixed.index):
            if getattr(df_fixed.index, "min", lambda: 0)() < INT64_MIN or getattr(df_fixed.index, "max", lambda: 0)() > INT64_MAX:
                print("Converting index to string (too large for int64).")
                df_fixed.index = df_fixed.index.astype(str)
    except Exception:
        df_fixed.index = df_fixed.index.astype(str)

    # --- Handle columns ---
    for col in df_fixed.columns:
        s = df_fixed[col]
        if pd.api.types.is_integer_dtype(s):
            if (s.min() < INT64_MIN) or (s.max() > INT64_MAX):
                print(f"Column '{col}' too large â€” converting to string.")
                df_fixed[col] = s.astype(str)
        elif pd.api.types.is_object_dtype(s):
            # Convert nested or large objects
            def clean(x):
                if isinstance(x, int) and not (INT64_MIN <= x <= INT64_MAX):
                    return str(x)
                if isinstance(x, (dict, list, tuple)):
                    return json.dumps(x)
                try:
                    str(x)
                    return x
                except Exception:
                    return repr(x)
            df_fixed[col] = s.map(clean).astype(str)
    return df_fixed