# -*- coding: utf-8 -*-
"""
Fit a 3-feature GLM to predict the spike rate of a neuron, given beahvioral inputs.

Functions
---------
fit_GLM()
    Fit a GLM for a 1D value of y (i.e., single cell).



TODO: At some point, modify the GLM so that multiple frames (in perserved
presered temporal sequence) can be used to predict a single frame's firing
rate.

Author: DMM, 2025
"""


from tqdm import tqdm
import numpy as np
from sklearn import preprocessing
from itertools import combinations


def fit_closed_GLM(X, y, usebias=True):
    """ Fit a GLM for a 1D value of y (i.e., single cell).
    
    """
    # y is the spike data for a single cell
    # X is a 2D array. for prediction using {pupil, retiocentric, egocentric}, there are
    # 3 features. So, shape should be {#frames, 3}.
    # w will be the bias followed 

    n_samples, n_features = X.shape

    if usebias:
        # Add bias (intercept) term: shape becomes (n_samples, num_features+1)
        # bias is inserted before any of the weights for individual behavior variables, so
        # X_aug should be {bias, w_p, w_r, w_e}
        X_aug = np.hstack([np.ones((n_samples, 1)), X])
    elif not usebias:
        X_aug = X

    # Closed-form solution: w = (X^T X)^(-1) X^T y
    XtX = X_aug.T @ X_aug
    Xty = X_aug.T @ y
    weights = np.linalg.inv(XtX) @ Xty
    
    return weights


def compute_y_hat(X, y, w):

    n_samples, n_features = X.shape

    # Was there a bias computed when the GLM was fit?
    if np.size(w)==n_features+1:
        usebias = True

    if usebias:
        # Add bias to the spike rate data
        X_aug = np.hstack([np.ones((n_samples, 1)), X])
    else:
        X_aug = X.copy()

    y_hat = X_aug @ w

    mse = np.mean((y - y_hat)**2)

    return y_hat, mse



class GLM:
    def __init__(
            self,
            learning_rate=0.001,
            epochs=5000,
            l1_penalty=0.01,
            l2_penalty=0.01,
        ):

        self.learning_rate = learning_rate
        self.epochs = epochs
        self.l1_penalty = l1_penalty
        self.l2_penalty = l2_penalty

        self.weights = None
        self.X_means = None
        self.X_stds = None

    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def _softplus(self, z):
        return np.log(1 + np.exp(z))
    
    def _zscore(self, X):

        X_z = np.zeros_like(X)
        savemeans = np.zeros(np.size(X,1))
        savestd = np.zeros(np.size(X,1))

        for feat in range(np.size(X,1)):
            mean_ = np.nanmean(X)
            std_ = np.nanstd(X)
            X_z[:, feat] = (X[:, feat] - mean_) / std_

        return X_z, savemeans, savestd
    
    def _apply_zscore(self, X):

        X_z = np.zeros_like(X)

        for feat in range(np.size(X,1)):
            X_z[:, feat] = (X[:, feat] - self.X_means[feat]) / self.X_stds[feat]

        return X_z

    def _loss(self, y_true, y_pred):

        m = len(y_true)
        log_loss = -np.mean(y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15))
        l1 = self.l1_penalty * np.sum(np.abs(self.weights[1:]))
        l2 = self.l2_penalty * np.sum(self.weights[1:] ** 2)

        return log_loss + l1 + l2
    
    def fit(self, X, y):

        self.weights = np.zeros(np.size(X,1)+1)

        # if y is 1D
        if len(np.shape(y)) != 2:
            y = y[:,np.newaxis]

        # scale values
        X_scaled, Xmeans, Xstds = self._zscore(X)
        self.X_means = Xmeans
        self.X_stds = Xstds

        # Add bias term
        X_bias = np.c_[np.ones(X_scaled.shape[0]), X_scaled]
        m = len(y)

        for epoch in range(self.epochs):
            z = np.dot(X_bias, self.weights)
            y_pred = self._softplus(z)

            gradient = np.dot(X_bias.T, (y_pred - y.flatten())) / m
            # Apply L2 regularization (ridge)
            gradient[1:] += self.l2_penalty * 2 * self.weights[1:]
            # Apply L1 regularization (lasso) - subgradient method
            gradient[1:] += self.l1_penalty * np.sign(self.weights)[1:]

            self.weights -= self.learning_rate * gradient

    def _predict(self, X):

        assert self.X_means is not None
        assert self.X_stds is not None

        X_scaled = self._apply_zscore(X)

        X_bias = np.c_[np.ones(X_scaled.shape[0]), X_scaled]
        y_hat = self._softplus(np.dot(X_bias, self.weights))[:, np.newaxis]

        return y_hat

    def score_explained_variance(self, y, y_hat):
        # Similar to r^2 except that this will treat an offset as error, whereas
        # r^2 does not penalize for an offset, it just treats it as an intercept.

        y_diff_avg = np.nanmean(y - y_hat, axis=0)
        n_ = np.nanmean((y - y_hat - y_diff_avg) ** 2, axis=0)
        y_null = np.nanmean(y, axis=0)
        d_ = np.nanmean((y - y_null)**2, axis=0)

        return n_ / d_
    
    def predict(self, X, y):
        # predict and score weights

        # if y is 1D
        if len(np.shape(y)) != 2:
            y = y[:,np.newaxis]

        # should be X_test and y_test as inputs
        y_hat = self._predict(X)
        
        mse = np.mean((y - y_hat)**2)
        explained_variance = self.score_explained_variance(y, y_hat)

        return y_hat, mse, explained_variance

    # def predict_with_dropout(self, X, y):
        # Try every combination of weights being set to 0 so that the model performance with or without
        # behavioral measures can be compared. should i do a version that drops out the bias term? not sure
        # what the biological interpretation would be of this...

        # Number of weights excluding the bias term
        # nW = len(self.weights) - 1

        # How many combinations should I try?

    def get_weights(self):
        return self.weights
    

def add_temporal_features(X, add_lags=1):

    nFrames, nFeats = np.shape(X)
    nFeatsOut = nFeats+(nFeats*add_lags)

    X_temporal = np.zeros([nFrames,nFeatsOut]) * np.nan

    print(nFeatsOut)

    i = 0
    for feat in range(nFeats):
        # Aligned data
        X_temporal[:,i] = X[:,feat].copy()
        i += 1

        for lag in range(1, add_lags+1):
            # Iterate through each lag position
            r = np.roll(X[:,feat].copy(), shift=-lag)
            r[lag:-lag] = np.nan
            X_temporal[:,i] = r
            i += 1

    # Drop the beginning and end of recording where there will be leftover NaNs
    X_temporal = X_temporal[add_lags:-add_lags]

    return X_temporal


def fit_pred_GLM(spikes, pupil, retino, ego, speed, opts=None):
    # spikes for a whole dataset of neurons, shape = {#frames, #cells}

    if opts is None:
        learning_rate = 0.001
        epochs = 5000
        l1_penalty = 0.01
        l2_penalty = 0.01
        num_lags = 10
    elif opts is not None:
        learning_rate = opts['learning_rate']
        epochs = opts['epochs']
        l1_penalty = opts['l1_penalty']
        l2_penalty = opts['l2_penalty']
        num_lags = opts['num_lags']

    # First, threshold all inputs by the animal's speed, i.e., drop
    # frames in which the animal is stationary
    speed = np.append(speed, speed[-1])
    use = speed > 1.5 # cm/sec

    spikes = spikes[use,:]
    pupil = pupil[use]
    retino = retino[use]
    ego = ego[use]

    _, nCells = np.shape(spikes)
    X_shared = np.stack([pupil, retino, ego], axis=1)

    # For each behavioral measure, add 9 previous time points so temporal
    # filters are learned. If it started w/ 3 features, will now have 30.
    if num_lags > 1:
        X_shared = add_temporal_features(X_shared, add_lags=num_lags)
        spikes = spikes[num_lags-1:-(num_lags-1), :]

    # Drop any frame for which one of the behavioral varaibles was NaN
    # At the end, need to compute y_hat and then add NaN indices back in so that temporal
    # structure of the origional recording is preseved.
    _keepFmask = ~np.isnan(X_shared).any(axis=1)
    X_shared_ = X_shared.copy()[_keepFmask,:]
    spikes_ = spikes.copy()[_keepFmask,:]

    nFrames = np.sum(_keepFmask)
    print(nFrames)

    # Make train/test split by splitting frames into 20 chunks,
    # shuffling the order of those chunks, and then grouping them
    # into two groups at a 75/25 ratio. Same timepoint split will
    # be used across all cells.
    ncnk = 20
    traintest_frac = 0.75
    cnk_sz = nFrames // ncnk
    _all_inds = np.arange(0,nFrames)
    chunk_order = np.arange(ncnk)
    np.random.shuffle(chunk_order)
    train_test_boundary = int(ncnk * traintest_frac)

    train_inds = []
    test_inds = []
    for cnk_i, cnk in enumerate(chunk_order):
        _inds = _all_inds[(cnk_sz*cnk) : ((cnk_sz*(cnk+1)))]
        if cnk_i < train_test_boundary:
            train_inds.extend(_inds)
        elif cnk_i >= train_test_boundary:
            test_inds.extend(_inds)
    train_inds = np.sort(np.array(train_inds)).astype(int)
    test_inds = np.sort(np.array(test_inds)).astype(int)

    # GLM weights for all cells
    w = np.zeros([
        nCells,
        np.size(X_shared_,1)+1    # number of features + a bias term
    ]) * np.nan
    # Predicted spike rate for the test data
    y_hat = np.zeros([
        nCells,
        len(test_inds)
    ]) * np.nan
    # Mean-squared error for each cell
    mse = np.zeros(nCells) * np.nan
    explvar = np.zeros(nCells) * np.nan

    X_train = X_shared_[train_inds, :].copy()
    X_test = X_shared_[test_inds, :].copy()

    for cell in tqdm(range(nCells)):

        y_train_c = spikes_[train_inds, cell].copy()
        y_test_c = spikes_[test_inds, cell].copy()

        cell_model = GLM(
            learning_rate=learning_rate,
            epochs=epochs,
            l1_penalty=l1_penalty,
            l2_penalty=l2_penalty,
        )

        cell_model.fit(X_train, y_train_c)

        y_hat_c, mse_c, explvar_ = cell_model.predict(X_test, y_test_c)

        w_c = cell_model.get_weights()

        w[cell,:] = w_c.copy()
        y_hat[cell,:] = y_hat_c.copy().flatten()
        mse[cell] = mse_c
        explvar[cell] = explvar_

        # Initialize model as a GLM with a Tweedie distribution.
        # model = linear_model.TweedieRegressor(
        #     alpha=0.01,
        #     power=0,
        #     max_iter=3000,
        #     tol=1e-5,
        #     fit_intercept=False
        # )
        # modelfit = model.fit(X_train.T, y_train)
        # _gz = modelfit.coef_[0]
        # _ret = modelfit.coef_[1]
        # _ego = modelfit.coef_[2]
        # weights_gz.append(_gz)
        # weights_ret.append(_ret)
        # weights_ego.append(_ego)
        # w = np.array([_gz, _ret, _ego])
        # pred = w @ X_test
        # scoreval = calc_score(y_test, pred)

    X_scaled = cell_model._apply_zscore(X_train)


    result = {
        'GLM_weights': w,
        'speeduse': use,
        'keepFmask': _keepFmask,
        'X': X_shared_,
        'y': spikes_,
        'train_inds': train_inds,
        'test_inds': test_inds,
        'y_test_hat': y_hat,
        'MSE': mse,
        'explvar': explvar,
        'X_scaled': X_scaled
    }

    return result


