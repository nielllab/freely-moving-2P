import torch
import numpy as np
import fm2p
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import argparse
import matplotlib.pyplot as plt
import os
from matplotlib.backends.backend_pdf import PdfPages
from tqdm import tqdm
import matplotlib.cm as cm

device = 'cuda' # torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BaseModel(nn.Module):
    def __init__(self, 
                    in_features, 
                    N_cells,
                    config,
                    ):
        super(BaseModel, self).__init__()

        self.config = config
        self.in_features = in_features
        self.N_cells = N_cells
        self.activation_type = config['activation_type']
        self.loss_type = config.get('loss_type', 'mse')
        
        self.hidden_size = config.get('hidden_size', 0)
        self.dropout_p = config.get('dropout', 0.0)

        if self.hidden_size > 0:
            layers = [
                nn.Linear(self.in_features, self.hidden_size),
                nn.BatchNorm1d(self.hidden_size),
                nn.ReLU()
            ]
            if self.dropout_p > 0:
                layers.append(nn.Dropout(p=self.dropout_p))
            layers.append(nn.Linear(self.hidden_size, self.N_cells))
            self.Cell_NN = nn.Sequential(*layers)
        else:
            self.Cell_NN = nn.Sequential(nn.Linear(self.in_features, self.N_cells,bias=True))

        self.activations = nn.ModuleDict({'SoftPlus':nn.Softplus(beta=0.5),
                                          'ReLU': nn.ReLU(),
                                          'Identity': nn.Identity(),
                                          'Sigmoid': nn.Sigmoid()})
        
        if self.config['initW'] == 'zero':
            for m in self.Cell_NN.modules():
                if isinstance(m, nn.Linear):
                    torch.nn.init.uniform_(m.weight, a=-1e-6, b=1e-6)
                    if m.bias is not None:
                        m.bias.data.fill_(1e-6)
        elif self.config['initW'] == 'normal':
            for m in self.Cell_NN.modules():
                if isinstance(m, nn.Linear):
                    torch.nn.init.normal_(m.weight, std=1/m.in_features)
        
        if isinstance(self.Cell_NN, nn.Sequential):
            self.Cell_NN[-1].bias.data.fill_(0.0)

        self.L1_alpha = config['L1_alpha']
        if self.L1_alpha != None:
            self.register_buffer('alpha',config['L1_alpha']*torch.ones(1))

      
    def init_weights(self,m):
        if isinstance(m, nn.Linear):
            torch.nn.init.uniform_(m.weight,a=-1e-6,b=1e-6)
            m.bias.data.fill_(1e-6)
        
    def forward(self, inputs, pos_inputs=None):
        output = self.Cell_NN(inputs)
        if self.activation_type is not None and self.activation_type in self.activations:
            ret = self.activations[self.activation_type](output)
        else:
            ret = output
        return ret

    def loss(self,Yhat, Y): 

        if self.loss_type == 'poisson':
            loss_vec = torch.mean(Yhat - Y * torch.log(Yhat + 1e-8), axis=0)
        else:
            loss_vec = torch.mean((Yhat-Y)**2,axis=0)

        if self.L1_alpha != None:
            l1_reg0 = torch.stack([torch.linalg.vector_norm(NN_params,ord=1) for name, NN_params in self.Cell_NN.named_parameters() if '0.weight' in name])
            loss_vec = loss_vec + self.alpha*(l1_reg0)
        return loss_vec

    def get_weights(self):
        return {k: v.detach().cpu().numpy() for k, v in self.Cell_NN.state_dict().items()}


class PositionGLM(BaseModel):
    def __init__(self, 
                    in_features, 
                    N_cells, 
                    config,
                    device=device):
        super(PositionGLM, self).__init__(in_features, N_cells, config)
        
        self.L1_alpha = config.get('L1_alpha_m')
        if self.L1_alpha is None:
             self.L1_alpha = config.get('L1_alpha')

        if self.L1_alpha is not None:
            self.register_buffer('alpha', self.L1_alpha * torch.ones(1))

    def forward(self, inputs, pos_inputs=None):

        return super(PositionGLM, self).forward(inputs)
    

def arg_parser():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('--Nepochs',            type=int,         default=5000)
    args = parser.parse_args()
    return vars(args)


def add_temporal_lags(X, lags):
    """
    Add temporal lags to the input features.
    Args:
        X (np.array): Input features of shape (n_samples, n_features)
        lags (list): List of integers representing lags (e.g., [-2, -1, 0, 1, 2])
    Returns:
        X_lagged (np.array): Lagged features of shape (n_samples, n_features * len(lags))
    """
    X_lagged = []
    for lag in lags:
        shifted = np.roll(X, shift=-lag, axis=0)
        if lag < 0: shifted[: -lag, :] = 0
        elif lag > 0: shifted[-lag :, :] = 0
        X_lagged.append(shifted)
    return np.concatenate(X_lagged, axis=1)


def load_position_data(data_input, modeltype='full', lags=None, use_abs=False, device=device):

    if isinstance(data_input, (str, Path)):
        data = fm2p.read_h5(data_input)
    else:
        data = data_input

    feature_names = []
    theta = data.get('theta_interp')
    phi = data.get('phi_interp')

    yaw = data.get('head_yaw_deg')

    roll = data.get('roll_twop_interp')
    pitch = data.get('pitch_twop_interp')

    gyro_x = data.get('gyro_x_twop_interp')
    gyro_y = data.get('gyro_y_twop_interp')
    gyro_z = data.get('gyro_z_twop_interp')

    eyeT = data['eyeT'][data['eyeT_startInd']:data['eyeT_endInd']]
    eyeT = eyeT - eyeT[0]

    if 'dPhi' not in data.keys():
        phi_full = np.rad2deg(data['phi'][data['eyeT_startInd']:data['eyeT_endInd']])
        dPhi  = np.diff(fm2p.interp_short_gaps(phi_full, 5)) / np.diff(eyeT)
        dPhi = np.roll(dPhi, -2)
        data['dPhi'] = dPhi

    if 'dTheta' not in data.keys():# and 'dEye' not in data.keys():
        theta_full = np.rad2deg(data['theta'][data['eyeT_startInd']:data['eyeT_endInd']])
        dTheta  = np.diff(fm2p.interp_short_gaps(theta_full, 5)) / np.diff(eyeT)
        dTheta = np.roll(dTheta, -2)
        data['dTheta'] = dTheta

        t = eyeT.copy()[:-1]
        t1 = t + (np.diff(eyeT) / 2)
        data['eyeT1'] = t1

    # elif 'dTheta' not in data.keys():
    #     data['dTheta'] = data['dEye'].copy()

    dTheta = fm2p.interp_short_gaps(data['dTheta'])
    # print(phi_full.shape, theta_full.shape, dTheta.shape)
    dTheta = fm2p.interpT(dTheta, data['eyeT1'], data['twopT'])
    dPhi = fm2p.interp_short_gaps(data['dPhi'])
    # print(dPhi.shape, data['eyeT1'].shape, data['twopT'].shape, eyeT.shape, dTheta.shape)
    dPhi = fm2p.interpT(dPhi, data['eyeT1'], data['twopT'])

    ltdk = data['ltdk_state_vec'].copy()
    
    valid_arrays = [x for x in [theta, phi, yaw, roll, pitch, ltdk, dTheta, dPhi, gyro_x, gyro_y, gyro_z] if x is not None]
    if not valid_arrays:
        raise ValueError("No valid data arrays found.")
    
    min_len = min(len(x) for x in valid_arrays)
    
    # spikes = data.get('norm_spikes')
    spikes = data.get('norm_dFF')
    if spikes is None:
        raise ValueError("norm_spikes not found in HDF5 file.")
    
    for c in range(np.size(spikes, 0)):
        spikes[c,:] = fm2p.convfilt(spikes[c,:], 10)
        
    min_len = min(min_len, spikes.shape[1])
    
    if theta is not None: theta = theta[:min_len]
    if phi is not None: phi = phi[:min_len]
    if yaw is not None: yaw = yaw[:min_len]
    if roll is not None: roll = roll[:min_len]
    if pitch is not None: pitch = pitch[:min_len]
    spikes = spikes[:, :min_len]
    spikes = spikes.T
    ltdk = ltdk[:min_len]
    if dTheta is not None: dTheta = dTheta[:min_len]
    if dPhi is not None: dPhi = dPhi[:min_len]
    if gyro_x is not None: gyro_x = gyro_x[:min_len]
    if gyro_y is not None: gyro_y = gyro_y[:min_len]
    if gyro_z is not None: gyro_z = gyro_z[:min_len]
    
    if modeltype == 'full':
        features = []
        names = []
        if theta is not None: features.append(theta); names.append('theta')
        if phi is not None: features.append(phi); names.append('phi')
        if yaw is not None: features.append(yaw); names.append('yaw')
        if roll is not None: features.append(roll); names.append('roll')
        if pitch is not None: features.append(pitch); names.append('pitch')
        if dTheta is not None: features.append(dTheta); names.append('dTheta')
        if dPhi is not None: features.append(dPhi); names.append('dPhi')
        if gyro_x is not None: features.append(gyro_x); names.append('gyro_x')
        if gyro_y is not None: features.append(gyro_y); names.append('gyro_y')
        if gyro_z is not None: features.append(gyro_z); names.append('gyro_z')
        
        X = np.stack(features, axis=1)
        feature_names = names
    elif modeltype == 'theta':
        X = np.stack([theta, dTheta], axis=1)
        feature_names = ['theta', 'dTheta']
    elif modeltype == 'theta_pos':
        X = theta[:, np.newaxis]
        feature_names = ['theta']
    elif modeltype == 'theta_vel':
        X = dTheta[:, np.newaxis]
        feature_names = ['dTheta']
    elif modeltype == 'phi':
        X = np.stack([phi, dPhi], axis=1)
        feature_names = ['phi', 'dPhi']
    elif modeltype == 'phi_pos':
        X = phi[:, np.newaxis]
        feature_names = ['phi']
    elif modeltype == 'phi_vel':
        X = dPhi[:, np.newaxis]
        feature_names = ['dPhi']
    elif modeltype == 'yaw':
        X = np.stack([yaw, gyro_z], axis=1)
        feature_names = ['yaw', 'gyro_z']
    elif modeltype == 'yaw_pos':
        X = yaw[:, np.newaxis]
        feature_names = ['yaw']
    elif modeltype == 'yaw_vel':
        X = gyro_z[:, np.newaxis]
        feature_names = ['gyro_z']
    elif modeltype == 'roll':
        X = np.stack([roll, gyro_x], axis=1)
        feature_names = ['roll', 'gyro_x']
    elif modeltype == 'roll_pos':
        X = roll[:, np.newaxis]
        feature_names = ['roll']
    elif modeltype == 'roll_vel':
        X = gyro_x[:, np.newaxis]
        feature_names = ['gyro_x']
    elif modeltype == 'pitch':
        X = np.stack([pitch, gyro_y], axis=1)
        feature_names = ['pitch', 'gyro_y']
    elif modeltype == 'pitch_pos':
        X = pitch[:, np.newaxis]
        feature_names = ['pitch']
    elif modeltype == 'pitch_vel':
        X = gyro_y[:, np.newaxis]
        feature_names = ['gyro_y']
    elif modeltype == 'velocity_only':
        features = []
        names = []
        if dTheta is not None: features.append(dTheta); names.append('dTheta')
        if dPhi is not None: features.append(dPhi); names.append('dPhi')
        if gyro_x is not None: features.append(gyro_x); names.append('gyro_x')
        if gyro_y is not None: features.append(gyro_y); names.append('gyro_y')
        if gyro_z is not None: features.append(gyro_z); names.append('gyro_z')
        X = np.stack(features, axis=1)
        feature_names = names
    elif modeltype == 'position_only':
        features = []
        names = []
        if theta is not None: features.append(theta); names.append('theta')
        if phi is not None: features.append(phi); names.append('phi')
        if yaw is not None: features.append(yaw); names.append('yaw')
        if roll is not None: features.append(roll); names.append('roll')
        if pitch is not None: features.append(pitch); names.append('pitch')
        X = np.stack(features, axis=1)
        feature_names = names
    elif modeltype == 'eyes_only':
        features = []
        names = []
        if theta is not None: features.append(theta); names.append('theta')
        if phi is not None: features.append(phi); names.append('phi')
        if dTheta is not None: features.append(dTheta); names.append('dTheta')
        if dPhi is not None: features.append(dPhi); names.append('dPhi')
        X = np.stack(features, axis=1)
        feature_names = names
    elif modeltype == 'head_only':
        features = []
        names = []
        if yaw is not None: features.append(yaw); names.append('yaw')
        if roll is not None: features.append(roll); names.append('roll')
        if pitch is not None: features.append(pitch); names.append('pitch')
        if gyro_x is not None: features.append(gyro_x); names.append('gyro_x')
        if gyro_y is not None: features.append(gyro_y); names.append('gyro_y')
        if gyro_z is not None: features.append(gyro_z); names.append('gyro_z')
        X = np.stack(features, axis=1)
        feature_names = names
    else:
        raise ValueError(f"Invalid modeltype: {modeltype}")
    
    X_mean = np.nanmean(X, axis=0)
    X_std = np.nanstd(X, axis=0)
    X_std[X_std == 0] = 1.0
    X = (X - X_mean) / X_std
    X[np.isnan(X)] = 0.0

    if use_abs:
        X = np.abs(X)

    if lags is not None:
        X = add_temporal_lags(X, lags)
    
    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)

    if np.isnan(spikes).any():
        spikes[np.isnan(spikes)] = 0.0
    
    spikes_mean = np.nanmean(spikes, axis=0)
    spikes_std = np.nanstd(spikes, axis=0)
    spikes_std[spikes_std == 0] = 1.0
    spikes = (spikes - spikes_mean) / spikes_std

    # print(f"Target (dF/F) stats (Z-scored) -- Mean: {np.nanmean(spikes):.4f}, Std: {np.nanstd(spikes):.4f}, Max: {np.nanmax(spikes):.4f}")
    Y_tensor = torch.tensor(spikes, dtype=torch.float32).to(device)
    
    return X_tensor, Y_tensor, feature_names


def setup_model_training(model,params,network_config):

    check_names = []
    param_list = []
    if params['train_shifter']:
        param_list.append({'params': list(model.shifter_nn.parameters()),'lr': network_config['lr_shift'],'weight_decay':.0001})
    for name,p in model.named_parameters():
        if params['ModelID']<2:
            if ('Cell_NN' in name):
                if ('weight' in name):
                    param_list.append({'params':[p],'lr':network_config['lr_w'],'weight_decay':network_config['L2_lambda']})
                elif ('bias' in name):
                    param_list.append({'params':[p],'lr':network_config['lr_b']})
                check_names.append(name)
        elif (params['ModelID']==2) | (params['ModelID']==3):
            if ('posNN' in name):
                if ('weight' in name):
                    param_list.append({'params':[p],'lr':network_config['lr_w'],'weight_decay':network_config['L2_lambda_m']})
                elif ('bias' in name):
                    param_list.append({'params':[p],'lr':network_config['lr_b']})
                check_names.append(name)

    if network_config['optimizer'].lower()=='adam':
        optimizer = optim.Adam(params=param_list)
    else:
        optimizer = optim.SGD(params=param_list)

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)
    return optimizer, scheduler


def train_position_model(data_input, config, modeltype='full', save_path=None, load_path=None, device=device):

    lags = config.get('lags', None)
    use_abs = config.get('use_abs', False)

    X, Y, feature_names = load_position_data(data_input, modeltype=modeltype, lags=lags, use_abs=use_abs, device=device)
    
    n_samples = X.shape[0]
    n_chunks = 20
    
    indices = np.arange(n_samples)
    chunks = np.array_split(indices, n_chunks)
    
    chunk_indices = np.arange(n_chunks)
    np.random.seed(42)
    np.random.shuffle(chunk_indices)
    
    split_idx = int(0.8 * n_chunks)
    train_indices = np.sort(np.concatenate([chunks[i] for i in chunk_indices[:split_idx]]))
    test_indices = np.sort(np.concatenate([chunks[i] for i in chunk_indices[split_idx:]]))
    
    train_idx = torch.tensor(train_indices, device=device)
    test_idx = torch.tensor(test_indices, device=device)
    
    X_train, Y_train = X[train_idx], Y[train_idx]
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    config['in_features'] = X.shape[1]
    config['Ncells'] = Y.shape[1]
    
    model = PositionGLM(config['in_features'], config['Ncells'], config, device=device)
    model.to(device)
    
    if load_path and os.path.exists(load_path):
        print(f"Loading model from {load_path}")
        model.load_state_dict(torch.load(load_path))
    else:
        params = {'ModelID': 0, 'Nepochs': config.get('Nepochs', 1000), 'train_shifter': False}
        optimizer, scheduler = setup_model_training(model, params, config)
        
        model.train()
        # print("Starting training...")
        for epoch in range(params['Nepochs']):
            optimizer.zero_grad()

            outputs = model(X_train)
            
            loss = model.loss(outputs, Y_train)

            loss.sum().backward()
            optimizer.step()
            
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(loss.sum())
                else:
                    scheduler.step()
            
            # if epoch % 100 == 0:
                # print(f"\rEpoch {epoch}/{params['Nepochs']}, Loss: {loss.sum().item():.4f}", end='', flush=True)
        # print('\n')
                
        # print("Training complete.")
        
        if save_path:
            torch.save(model.state_dict(), save_path)
            # print(f"Model saved to {save_path}")
        
    return model, X_test, Y_test, feature_names


def test_position_model(model, X_test, Y_test):
    """
    Test the PositionGLM model and return the loss.
    """
    model.eval()
    with torch.no_grad():
        outputs = model(X_test)
        loss = model.loss(outputs, Y_test)
        mse = torch.mean((outputs - Y_test)**2).item()
        
    # print(f"Test Loss: {loss.sum().item():.4f}")
    # print(f"Avg MSE per cell: {mse:.4f}")
    return loss.sum().item()


def compute_permutation_importance(model, X_test, Y_test, feature_names, lags, device=device):
    
    model.eval()
    
    X_np = X_test.cpu().numpy()
    Y_np = Y_test.cpu().numpy()
    
    n_samples, n_inputs = X_np.shape
    n_lags = len(lags) if lags is not None else 1
    n_base_features = n_inputs // n_lags
    
    with torch.no_grad():
        y_hat = model(X_test).cpu().numpy()
    
    baseline_r2 = np.zeros(Y_np.shape[1])
    for c in range(Y_np.shape[1]):
        ss_res = np.sum((Y_np[:, c] - y_hat[:, c]) ** 2)
        ss_tot = np.sum((Y_np[:, c] - np.mean(Y_np[:, c])) ** 2)
        baseline_r2[c] = 1 - (ss_res / (ss_tot + 1e-8))
        
    importances = {}
    
    for i, feat_name in enumerate(feature_names):
        X_shuff = X_np.copy()
        
        for l in range(n_lags):
            col_idx = i + (l * n_base_features)
            np.random.shuffle(X_shuff[:, col_idx])
            
        X_shuff_tensor = torch.tensor(X_shuff, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            y_hat_shuff = model(X_shuff_tensor).cpu().numpy()
            
        shuff_r2 = np.zeros(Y_np.shape[1])
        for c in range(Y_np.shape[1]):
            ss_res = np.sum((Y_np[:, c] - y_hat_shuff[:, c]) ** 2)
            ss_tot = np.sum((Y_np[:, c] - np.mean(Y_np[:, c])) ** 2)
            shuff_r2[c] = 1 - (ss_res / (ss_tot + 1e-8))
            
        importances[feat_name] = baseline_r2 - shuff_r2
        
    return importances


def plot_feature_importance(data, model_key=None, cell_idx=None, save_path=None, show=True):

    if model_key is not None:

        importances = {}
        prefix = f'{model_key}_importance_'
        for k, v in data.items():
            if k.startswith(prefix):
                feat_name = k[len(prefix):]
                importances[feat_name] = v
        
        if not importances:
            print(f"No importance keys found for model '{model_key}' in data.")
            return
    else:
        importances = data

    feature_names = list(importances.keys())
    
    cmap = cm.get_cmap('tab20')
    colors = [cmap(i/len(feature_names)) for i in range(len(feature_names))]
    if hasattr(cmap, 'colors'):
        colors = [cmap.colors[i % 20] for i in range(len(feature_names))]
    else:
        colors = [cmap(i / 20) for i in range(len(feature_names))]
    colors = colors[:6] + colors[8:]
    
    if save_path and str(save_path).endswith('.pdf'):
        if model_key is None:
            print("model_key is required for PDF generation to sort by performance.")
            return

        corrs = data.get(f'{model_key}_corrs')
        if corrs is None:
            corrs = data.get(f'{model_key}_r2')
            
        if corrs is not None:
            sorted_indices = np.argsort(corrs)[::-1]
        else:
            n_cells = len(next(iter(importances.values())))
            sorted_indices = np.arange(n_cells)
            
        with PdfPages(save_path) as pdf:

            plt.figure(figsize=(8, 5), dpi=300)
            ax = plt.gca()
            for i, feat in enumerate(feature_names):
                vals = np.asarray(importances[feat]).flatten()
                fm2p.add_scatter_col(ax, i, vals)#, color=colors[i])
            
            plt.ylabel('Importance (Drop in R²)', fontsize=12)
            plt.title(f'Feature Importance Population Summary ({model_key})', fontsize=14)
            plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right', fontsize=12)
            plt.axhline(0, color='black', linewidth=0.8)
            plt.tight_layout()
            pdf.savefig()
            plt.close()
            
            for i, c_idx in enumerate(sorted_indices):
                plot_feature_importance(importances, cell_idx=c_idx, save_path=None, show=False)
                plt.suptitle(f'Cell {c_idx} (Rank {i+1}) - Corr: {corrs[c_idx]:.3f}', fontsize=12)
                pdf.savefig()
                plt.close()
        return

    if cell_idx is not None:

        n_cells = len(next(iter(importances.values())))
        if cell_idx >= n_cells:
            print(f"Cell index {cell_idx} out of bounds (n_cells={n_cells})")
            return

        values = [importances[feat][cell_idx] for feat in feature_names]
        
        plt.figure(figsize=(6, 4), dpi=300)
        bars = plt.bar(feature_names, values, color=colors, edgecolor='black')
        plt.ylabel('Importance (Drop in R²)', fontsize=12)
        # plt.title(f'Feature Importance for Cell {cell_idx}', fontsize=14)
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.axhline(0, color='black', linewidth=0.8)
        plt.grid(False)
        
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                     f'{height:.3f}',
                     ha='center', va='bottom' if height > 0 else 'top', fontsize=9)
                     
        plt.tight_layout()
        
    else:

        plt.figure(figsize=(8, 5), dpi=300)
        ax = plt.gca()
        for i, feat in enumerate(feature_names):
            vals = np.asarray(importances[feat]).flatten()
            fm2p.add_scatter_col(ax, i, vals, color=colors[i])
            fm2p.add_scatter_col(ax, i, vals, color=colors[i % len(colors)])
            
        plt.ylabel('Importance (Drop in R²)', fontsize=12)
        plt.title('Feature Importance Across All Cells', fontsize=14)
        plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right', fontsize=12)
        plt.axhline(0, color='black', linewidth=0.8)
        plt.tight_layout()

    if save_path:
        plt.savefig(save_path)
        plt.close()
    elif show:
        plt.show()


def plot_feature_importance_full(data, importances, save_path=None, show=True):

    feature_names = list(importances.keys())
    
    cmap = cm.get_cmap('tab20')
    colors = [cmap(i/len(feature_names)) for i in range(len(feature_names))]
    
    corrs = data.get('corrs')
    if corrs is None:
        corrs = data.get('r2_scores')
            
    if corrs is not None:
        sorted_indices = np.argsort(corrs)[::-1]
    else:
        n_cells = len(next(iter(importances.values())))
        sorted_indices = np.arange(n_cells)
            
    if save_path and str(save_path).endswith('.pdf'):
            
        with PdfPages(save_path) as pdf:

            plt.figure(figsize=(8, 5), dpi=300)
            ax = plt.gca()
            for i, feat in enumerate(feature_names):
                vals = np.asarray(importances[feat]).flatten()
                fm2p.add_scatter_col(ax, i, vals, color=colors[i])
            
            plt.ylabel('Importance (Drop in R²)', fontsize=12)
            plt.title(f'Feature Importance Population Summary (Full Model)', fontsize=14)
            plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right', fontsize=12)
            plt.axhline(0, color='black', linewidth=0.8)
            plt.tight_layout()
            pdf.savefig()
            plt.close()
            
            for i, c_idx in enumerate(sorted_indices):
                plot_feature_importance(importances, cell_idx=c_idx, save_path=None, show=False)
                plt.suptitle(f'Cell {c_idx} (Rank {i+1}) - Corr: {corrs[c_idx]:.3f}')
                pdf.savefig()
                plt.close()
    else:
        plot_feature_importance(importances, save_path=save_path, show=show)


def plot_feature_importance_full(data, importances, save_path=None, show=True):

    feature_names = list(importances.keys())
    
    cmap = cm.get_cmap('tab20')
    colors = [cmap(i/len(feature_names)) for i in range(len(feature_names))]
    
    corrs = data.get('corrs')
    if corrs is None:
        corrs = data.get('r2_scores')
            
    if corrs is not None:
        sorted_indices = np.argsort(corrs)[::-1]
    else:
        n_cells = len(next(iter(importances.values())))
        sorted_indices = np.arange(n_cells)
            
    if save_path and str(save_path).endswith('.pdf'):
            
        with PdfPages(save_path) as pdf:

            plt.figure(figsize=(8, 5), dpi=300)
            ax = plt.gca()
            for i, feat in enumerate(feature_names):
                vals = np.asarray(importances[feat]).flatten()
                fm2p.add_scatter_col(ax, i, vals)
            
            plt.ylabel('Importance (Drop in R²)', fontsize=12)
            plt.title(f'Feature Importance Population Summary (Full Model)', fontsize=14)
            plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right', fontsize=12)
            plt.axhline(0, color='black', linewidth=0.8)
            plt.tight_layout()
            pdf.savefig()
            plt.close()
            
            for i, c_idx in enumerate(sorted_indices):
                plot_feature_importance(importances, cell_idx=c_idx, save_path=None, show=False)
                plt.suptitle(f'Cell {c_idx} (Rank {i+1}) - Corr: {corrs[c_idx]:.3f}')
                pdf.savefig()
                plt.close()
    else:
        plot_feature_importance(importances, save_path=save_path, show=show)


def plot_shuffled_comparison(model, X_test, Y_test, feature_names, lags, feature_to_shuffle, cell_idx, save_path=None, device=device, pdf=None):

    model.eval()
    
    X_np = X_test.cpu().numpy()
    Y_np = Y_test.cpu().numpy()
    
    with torch.no_grad():
        y_hat = model(X_test).cpu().numpy()
        
    n_lags = len(lags) if lags is not None else 1
    n_inputs = X_np.shape[1]
    n_base_features = n_inputs // n_lags
    
    if feature_to_shuffle not in feature_names:
        print(f"Feature {feature_to_shuffle} not found in {feature_names}")
        return

    feat_idx = feature_names.index(feature_to_shuffle)
    X_shuff = X_np.copy()
    
    for l in range(n_lags):
        col_idx = feat_idx + (l * n_base_features)
        np.random.shuffle(X_shuff[:, col_idx])
        
    X_shuff_tensor = torch.tensor(X_shuff, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        y_hat_shuff = model(X_shuff_tensor).cpu().numpy()
        
    plt.figure(figsize=(12, 6))
    plot_len = min(1000, Y_np.shape[0])
    t = np.arange(plot_len)
    
    plt.plot(t, Y_np[:plot_len, cell_idx], 'k', label='True Spikes', alpha=0.4, linewidth=1)
    plt.plot(t, y_hat[:plot_len, cell_idx], 'b', label='Baseline Pred', linewidth=1.5, alpha=0.8)
    plt.plot(t, y_hat_shuff[:plot_len, cell_idx], 'r--', label=f'Shuffled {feature_to_shuffle} Pred', linewidth=1.5, alpha=0.8)
    
    plt.title(f'Effect of Shuffling {feature_to_shuffle} on Cell {cell_idx}\n(Red line better than Blue = Negative Importance)')
    plt.xlabel('Time (frames)')
    plt.ylabel('Activity (z-scored)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path)
        plt.close()
    elif pdf:
        pdf.savefig()
        plt.close()
    else:
        plt.show()


def save_shuffled_comparison_pdf(model, X_test, Y_test, feature_names, lags, importances, corrs, save_path, device=device):
    sorted_indices = np.argsort(corrs)[::-1]
    
    with PdfPages(save_path) as pdf:

        for i, cell_idx in enumerate(sorted_indices):

            neg_feats = []
            for feat, imp in importances.items():
                if imp[cell_idx] < -0.05:
                    neg_feats.append((feat, imp[cell_idx]))
            
            neg_feats.sort(key=lambda x: x[1])
            
            for feat, imp in neg_feats:
                plot_shuffled_comparison(
                    model,
                    X_test,
                    Y_test,
                    feature_names,
                    lags,
                    feat,
                    cell_idx,
                    save_path=None,
                    device=device,
                    pdf=pdf
                )


def save_model_predictions_pdf(dict_out, save_path):
    
    if 'full_r2' in dict_out:
        r2 = dict_out['full_r2']
        sorted_indices = np.argsort(r2)[::-1]
    else:
        n_cells = dict_out['full_y_hat'].shape[1]
        sorted_indices = np.arange(n_cells)

    # all_keys = [k.replace('_y_hat', '') for k in dict_out.keys() if k.endswith('_y_hat')]
    normal_models = [
        'theta_y_hat',
        'phi_y_hat',
        'yaw_y_hat',
        'roll_y_hat',
        'pitch_y_hat',
        'dTheta_y_hat',
        'dPhi_y_hat',
        'gyro_z_y_hat',
        'gyro_x_y_hat',
        'gyro_y_y_hat'
    ]
    
    # normal_models = [k for k in all_keys if 'abs' not in k and k != 'full']
    # abs_models = [k for k in all_keys if 'abs' in k and k != 'full_abs']
    
    # has_full = 'full' in all_keys
    # has_full_abs = 'full_abs' in all_keys

    with PdfPages(save_path) as pdf:
        for cell_idx in tqdm(sorted_indices, desc="Generating Predictions PDF"):
            
            y_true = dict_out['full_y_true'][:, cell_idx]
            t = np.arange(len(y_true))
            
            fig = plt.figure(figsize=(12, 8), dpi=300)
            gs = fig.add_gridspec(3, 5)
            ax_main = fig.add_subplot(gs[0, :])
            
            ax_main.plot(t, y_true, 'k', alpha=0.5, label='True')
            # if has_full:
            y_pred_full = dict_out['full_y_hat'][:, cell_idx]
            ax_main.plot(t, y_pred_full, 'r', alpha=0.7, label='Full Model')
            r2_val = dict_out['full_r2'][cell_idx]
            ax_main.set_title(f'Cell {cell_idx} - Full Model (R2={r2_val:.3f})')
            ax_main.legend()
            
            for i, model in enumerate(normal_models):
                row = 1 + i // 4
                col = i % 4
                if row < 3:
                    ax = fig.add_subplot(gs[row, col])
                    y_pred = dict_out[f'{model}_y_hat'][:, cell_idx]
                    ax.plot(t, y_true, 'k', alpha=0.3)
                    ax.plot(t, y_pred, 'b', alpha=0.7)
                    r2_m = dict_out[f'{model}_r2'][cell_idx]
                    ax.set_title(f'{model} (R2={r2_m:.3f})', fontsize=8)
                    ax.axis('off')
            
            plt.tight_layout()
            pdf.savefig(fig)
            plt.close(fig)
            
            # fig = plt.figure(figsize=(12, 8), dpi=300)
            # gs = fig.add_gridspec(3, 4)
            # ax_main = fig.add_subplot(gs[0, :])
            
            # ax_main.plot(t, y_true, 'k', alpha=0.5, label='True')
            # if has_full_abs:
            #     y_pred_full = dict_out['full_abs_y_hat'][:, cell_idx]
            #     ax_main.plot(t, y_pred_full, 'r', alpha=0.7, label='Full Abs Model')
            #     r2_val = dict_out['full_abs_r2'][cell_idx]
            #     ax_main.set_title(f'Cell {cell_idx} - Full Abs Model (R2={r2_val:.3f})')
            # ax_main.legend()
            
            # for i, model in enumerate(abs_models):
            #     row = 1 + i // 4
            #     col = i % 4
            #     if row < 3:
            #         ax = fig.add_subplot(gs[row, col])
            #         y_pred = dict_out[f'{model}_y_hat'][:, cell_idx]
            #         ax.plot(t, y_true, 'k', alpha=0.3)
            #         ax.plot(t, y_pred, 'g', alpha=0.7)
            #         r2_m = dict_out[f'{model}_r2'][cell_idx]
            #         ax.set_title(f'{model} (R2={r2_m:.3f})', fontsize=8)
            #         ax.axis('off')
            
            # plt.tight_layout()
            # pdf.savefig(fig)
            # plt.close(fig)


def fit_test_pytorchGLM(data_input, save_dir=None):

    if isinstance(data_input, (str, Path)):
        if save_dir is None:
            save_dir = os.path.split(data_input)[0]

    data = fm2p.check_and_trim_imu_disconnect(data_input)

    base_path = save_dir

    pos_config = {
        'activation_type': 'Identity',
        'loss_type': 'mse',
        'initW': 'normal',
        'optimizer': 'adam',
        'lr_w': 1e-2, 
        'lr_b': 1e-2,
        'L1_alpha': 1e-2,
        'Nepochs': 5000,
        'L2_lambda': 1e-3,
        'lags': np.arange(-4,1,1), # was -10 (1.33 sec)
        'use_abs': False,
        'hidden_size': 128,
        'dropout': 0.25
    }

    print(f"Fitting full model")
    full_model_path = os.path.join(base_path, 'full_model.pth')
    model, X_test, y_test, feature_names = train_position_model(data, pos_config, save_path=full_model_path, load_path=full_model_path)

    loss = test_position_model(model, X_test, y_test)

    model.eval()
    with torch.no_grad():
        y_hat = model(X_test)
    
    y_true = y_test.cpu().numpy()
    y_pred = y_hat.cpu().numpy()
    
    n_cells = y_true.shape[1]
    r2_scores = np.zeros(n_cells)
    
    for c in range(n_cells):
        ss_res = np.sum((y_true[:, c] - y_pred[:, c]) ** 2)
        ss_tot = np.sum((y_true[:, c] - np.mean(y_true[:, c])) ** 2)
        r2_scores[c] = 1 - (ss_res / (ss_tot + 1e-8))

    corrs = np.zeros(np.size(y_true,1))
    for c in range(np.size(y_true,1)):
        corrs[c] = fm2p.corrcoef(y_true[:,c], y_pred[:,c])
            
    best_cell_idx = np.argmax(r2_scores)
    # print(f"Best cell index: {best_cell_idx}, R2: {r2_scores[best_cell_idx]:.4f}")
    
    # plt.figure(figsize=(15, 5))
    # plt.plot(y_true[:, best_cell_idx], 'k', label='True', linewidth=1)
    # plt.plot(y_pred[:, best_cell_idx], 'r', label='Predicted', alpha=0.7, linewidth=1)
    # plt.title(f'Best Cell (Index {best_cell_idx}) - R^2 = {r2_scores[best_cell_idx]:.3f}')
    # plt.legend()
    # plt.show()
    
    # plt.figure(figsize=(8, 6))
    # plt.hist(r2_scores, bins=20, edgecolor='k', alpha=0.7)
    # plt.xlabel('R^2 Score')
    # plt.ylabel('Count')
    # plt.title('Distribution of R^2 Scores')
    # plt.show()

    dict_out = {
        'full_r2': r2_scores,
        'full_corrs': corrs,
        'full_y_hat': y_pred,
        'full_y_true': y_true,
        'full_weights': model.get_weights()
    }

    importances = compute_permutation_importance(model, X_test, y_test, feature_names, pos_config.get('lags'))
    for feat, imp in importances.items():
        dict_out[f'full_importance_{feat}'] = imp
        
    # fi_pdf_path = os.path.join(base_path, 'full_feature_importance.pdf')
    # plot_feature_importance_full(dict_out, importances, save_path=fi_pdf_path)
    
    # sc_pdf_path = os.path.join(base_path, 'full_shuffled_comparison.pdf')
    # save_shuffled_comparison_pdf(model, X_test, y_test, feature_names, pos_config.get('lags'), importances, corrs, sc_pdf_path, device=device)

    # existing_data = fm2p.read_h5(os.path.join(base_path, 'pytorchGLM_predictions_v03_add_shuff.h5'))
    # newdata = {**existing_data, **dict_out}
    # fm2p.write_h5(
    #     os.path.join(base_path, 'pytorchGLM_predictions_v03A_add_full_FI.h5'),
    #     newdata
    # )
    model_runs = []
    
    model_runs.append({'key': 'velocity_only', 'type': 'velocity_only', 'abs': False})
    model_runs.append({'key': 'position_only', 'type': 'position_only', 'abs': False})
    model_runs.append({'key': 'eyes_only', 'type': 'eyes_only', 'abs': False})
    model_runs.append({'key': 'head_only', 'type': 'head_only', 'abs': False})

    for run in model_runs:
        key = run['key']
        mtype = run['type']
        use_abs = run['abs']

        print(f'Fitting model: {key} (type={mtype}, abs={use_abs})')

        current_config = pos_config.copy()
        current_config['use_abs'] = use_abs

        model, X_test, y_test, feature_names = train_position_model(data, current_config, modeltype=mtype)
        loss = test_position_model(model, X_test, y_test)

        model.eval()
        with torch.no_grad():
            y_hat = model(X_test)
        
        y_true = y_test.cpu().numpy()
        y_pred = y_hat.cpu().numpy()
        
        n_cells = y_true.shape[1]
        r2_scores = np.zeros(n_cells)
        
        for c in range(n_cells):
            ss_res = np.sum((y_true[:, c] - y_pred[:, c]) ** 2)
            ss_tot = np.sum((y_true[:, c] - np.mean(y_true[:, c])) ** 2)
            r2_scores[c] = 1 - (ss_res / (ss_tot + 1e-8))

        corrs = np.zeros(np.size(y_true,1))
        for c in range(np.size(y_true,1)):
            corrs[c] = fm2p.corrcoef(y_true[:,c], y_pred[:,c])

        dict_out[f'{key}_r2'] = r2_scores
        dict_out[f'{key}_corrs'] = corrs
        dict_out[f'{key}_y_hat'] = y_pred
        dict_out[f'{key}_y_true'] = y_true
        dict_out[f'{key}_weights'] = model.get_weights()
        
        importances = compute_permutation_importance(model, X_test, y_test, feature_names, current_config.get('lags'))
        for feat, imp in importances.items():
            dict_out[f'{key}_importance_{feat}'] = imp
            
        # best_cell_idx = np.argmax(r2_scores)
        
        # fi_pdf_path = os.path.join(base_path, f'{key}_feature_importance.pdf')
        # plot_feature_importance(dict_out, model_key=key, save_path=fi_pdf_path)
        
        # sc_pdf_path = os.path.join(base_path, f'{key}_shuffled_comparison.pdf')
        # save_shuffled_comparison_pdf(model, X_test, y_test, feature_names, current_config.get('lags'), importances, corrs, sc_pdf_path, device=device)

    h5_savepath = os.path.join(base_path, 'pytorchGLM_predictions_v07_multidropout.h5')
    # print('Writing to {}'.format(h5_savepath))
    fm2p.write_h5(h5_savepath, dict_out)

    # save_model_predictions_pdf(dict_out, os.path.join(base_path, 'all_model_predictions.pdf'))


def pytorchGLM():

    # cohort_dir = '/home/dylan/Storage/freely_moving_data/_V1PPC/cohort02_recordings/cohort02_recordings/'
    # cohort_dir = '/home/dylan/Storage/freely_moving_data/_V1PPC/cohort01_recordings/'
    # recordings = fm2p.find(
    #     '*fm*_preproc.h5',
    #     cohort_dir
    # )
    # print('Found {} recordings.'.format(len(recordings)))

    # recordings = recordings[7:]

    # for ri, rec in enumerate(recordings):

    #     print('Fitting models for recordings {} of {} ({}).'.format(ri+1, len(recordings), rec))

    #     fit_test_pytorchGLM(rec)

    fit_test_pytorchGLM(
        '/home/dylan/Storage/freely_moving_data/_V1PPC/cohort02_recordings/cohort02_recordings/251021_DMM_DMM061_pos04/fm1/251021_DMM_DMM061_fm_01_preproc.h5'
    )


if __name__ == '__main__':

    pytorchGLM()
